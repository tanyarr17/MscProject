{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Install and Import libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install vaderSentiment\n\nimport numpy as np \nimport pandas as pd\nimport re\nimport io\nimport requests\nimport string\nimport operator\n\nfrom tqdm import tqdm\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nfrom sklearn.metrics import accuracy_score,auc,f1_score,recall_score,precision_score,classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom functools import partial\n\nfrom bs4 import BeautifulSoup\nimport html as ihtml\n\nimport spacy\nimport en_core_web_sm\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":57,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: vaderSentiment in /opt/conda/lib/python3.6/site-packages (3.2.1)\n['imdb-movie-reviews-dataset']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Import Data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/imdb-movie-reviews-dataset/aclimdb/aclImdb/\"\npositiveFiles = [x for x in os.listdir(path+\"train/pos/\") if x.endswith(\".txt\")]\nnegativeFiles = [x for x in os.listdir(path+\"train/neg/\") if x.endswith(\".txt\")]\npositiveReviews, negativeReviews = [], []\nfor pfile in positiveFiles:\n    with open(path+\"train/pos/\"+pfile) as f:\n        positiveReviews.append(f.read())\nfor nfile in negativeFiles:\n    with open(path+\"train/neg/\"+nfile) as f:\n        negativeReviews.append(f.read())\n        \nreviews = pd.concat([\n    pd.DataFrame({\"review\":positiveReviews, \"label\":1, \"file\":positiveFiles}),\n    pd.DataFrame({\"review\":negativeReviews, \"label\":0, \"file\":negativeFiles})\n], ignore_index=True).sample(frac=1, random_state=1)\nreviews.head()","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"                                                  review  label         file\n21492  Just plain good old stupid. <br /><br />I mean...      0   9552_1.txt\n9488   There are too many new styles of the sitcom bu...      1   3158_8.txt\n16933  Contrary to most other comments about \"Syriana...      0  12461_1.txt\n12604  I'm surprised this movie is rated so highly, a...      0  12189_4.txt\n8222   Sergeant Ryker is accused of being a traitor d...      1   8584_8.txt","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21492</th>\n      <td>Just plain good old stupid. &lt;br /&gt;&lt;br /&gt;I mean...</td>\n      <td>0</td>\n      <td>9552_1.txt</td>\n    </tr>\n    <tr>\n      <th>9488</th>\n      <td>There are too many new styles of the sitcom bu...</td>\n      <td>1</td>\n      <td>3158_8.txt</td>\n    </tr>\n    <tr>\n      <th>16933</th>\n      <td>Contrary to most other comments about \"Syriana...</td>\n      <td>0</td>\n      <td>12461_1.txt</td>\n    </tr>\n    <tr>\n      <th>12604</th>\n      <td>I'm surprised this movie is rated so highly, a...</td>\n      <td>0</td>\n      <td>12189_4.txt</td>\n    </tr>\n    <tr>\n      <th>8222</th>\n      <td>Sergeant Ryker is accused of being a traitor d...</td>\n      <td>1</td>\n      <td>8584_8.txt</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Pre-process**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TransformerMixin is used for creating our own pipeline/class for a procedure\nclass CleanText(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return X.apply(lambda text: clean_text(text))\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def get_params(self, deep=True):\n        return {}\n    \ndef clean_text(text):\n    text = BeautifulSoup(ihtml.unescape(text)).text\n    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r\"[^a-zA-Z ]\",\" \",text)\n    return text\n\nnlp = spacy.load('en_core_web_lg', parse=True, tag=True, entity=True)\npunctuations = string.punctuation\n\n# stop words for filtering\nstopwords = list(STOP_WORDS)\nparser = English()","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ExtractFeatures(TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, reviews, y=None):\n        return self\n    \n    def get_params(self, **kwargs):\n        return {}\n        \n    def extract_features(self, X):\n        doc = nlp(X,disable=['parser', 'ner'])\n        word_count = len(doc)\n        \n        # Creating a tokenized review text\n        mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc ]\n         # Removing stop words\n        mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n        parsed_review = \" \".join(mytokens)\n        \n        return pd.Series({'parsed_review': parsed_review, \n                          'word_count': word_count\n                         })\n       \n    def transform(self, X, y=None):\n        return X.apply(lambda text: self.extract_features(text), 1)","execution_count":60,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def parse_column(X):\n    return X['parsed_review']\ndef wc_column(X):\n    return X.loc[:,'word_count'].values[:,np.newaxis]","execution_count":61,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TFIDF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_pipeline = make_pipeline(\n    # Clean the text\n        CleanText(),\n    #Feature extraction\n        ExtractFeatures(),\n        make_union(\n            make_pipeline(\n                FunctionTransformer(parse_column,validate=False),\n                TfidfVectorizer(sublinear_tf=True, min_df=0.0025, max_df = 0.4,\n                        ngram_range=(1, 3), \n                        stop_words=stopwords)\n            ),\n            FunctionTransformer(wc_column,validate=False)\n        ),\n)\n\ntfidf_prepipeline = make_column_transformer(\n    (tfidf_pipeline,'review')\n)","execution_count":62,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bag of Words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_pipeline = make_pipeline(\n    # Clean the text\n        CleanText(),\n    #Feature extraction\n        ExtractFeatures(),\n        make_union(\n            make_pipeline(\n                FunctionTransformer(parse_column,validate=False),\n                CountVectorizer(ngram_range=(1,1))\n            ),\n            FunctionTransformer(wc_column,validate=False)\n        ),\n)\n\nbow_prepipeline = make_column_transformer(\n    (bow_pipeline,'review')\n)","execution_count":63,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train and Test Split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = reviews[reviews.label.isin([0,1])]\nX_train, X_test, Y_train, Y_test = train_test_split(train.drop('label',1), train.label)\nX_train.reset_index(inplace=True,drop=True)","execution_count":64,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression (TFIDF & BoW)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model = Pipeline([\n    (\"preprocessor\", tfidf_prepipeline),\n    (\"svd\", TruncatedSVD(n_components=1000, n_iter=7, random_state=42)),\n    (\"model\", LogisticRegression(class_weight='balanced', solver='liblinear'))\n])\n\nlr_model2 = Pipeline([\n    (\"preprocessor\", bow_prepipeline),\n    (\"svd\", TruncatedSVD(n_components=1000, n_iter=7, random_state=42)),\n    (\"model\", LogisticRegression(class_weight='balanced', solver='liblinear'))\n])","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model.fit(X_train, Y_train)\nY_pred = lr_model.predict(X_test)\nlracc = accuracy_score(Y_test, Y_pred)\nlrfs = f1_score(Y_test, Y_pred,pos_label=1)\nprint(classification_report(Y_test, Y_pred))","execution_count":66,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.88      0.85      0.87      3135\n           1       0.86      0.89      0.87      3115\n\n    accuracy                           0.87      6250\n   macro avg       0.87      0.87      0.87      6250\nweighted avg       0.87      0.87      0.87      6250\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_model2.fit(X_train, Y_train)\nY_pred = lr_model2.predict(X_test)\nlracc2 = accuracy_score(Y_test, Y_pred)\nlrfs2 = f1_score(Y_test, Y_pred,pos_label=1)\nprint(classification_report(Y_test, Y_pred))","execution_count":67,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.87      0.85      0.86      3135\n           1       0.85      0.87      0.86      3115\n\n    accuracy                           0.86      6250\n   macro avg       0.86      0.86      0.86      6250\nweighted avg       0.86      0.86      0.86      6250\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**RandomForest (TFIDF & BoW)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model = Pipeline([\n    (\"preprocessor\", tfidf_prepipeline),\n    (\"svd\", TruncatedSVD(n_components=100, n_iter=7, random_state=42)),\n    (\"model\", RandomForestClassifier(class_weight='balanced', n_estimators=500, n_jobs=-1))\n])\n\nrf_model2 = Pipeline([\n    (\"preprocessor\", bow_prepipeline),\n    (\"svd\", TruncatedSVD(n_components=100, n_iter=7, random_state=42)),\n    (\"model\", RandomForestClassifier(class_weight='balanced', n_estimators=100, n_jobs=-1))\n])","execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model.fit(X_train, Y_train)\nY_pred = rf_model.predict(X_test)\nrfacc = accuracy_score(Y_test, Y_pred)\nrffs = f1_score(Y_test, Y_pred,pos_label=1)\nprint(classification_report(Y_test, Y_pred))","execution_count":88,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.83      0.79      0.81      3135\n           1       0.80      0.83      0.82      3115\n\n    accuracy                           0.81      6250\n   macro avg       0.81      0.81      0.81      6250\nweighted avg       0.81      0.81      0.81      6250\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model2.fit(X_train, Y_train)\nY_pred = rf_model2.predict(X_test)\nrfacc2 = accuracy_score(Y_test, Y_pred)\nrffs2 = f1_score(Y_test, Y_pred,pos_label=1)\nprint(classification_report(Y_test, Y_pred))","execution_count":70,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.78      0.72      0.75      3135\n           1       0.74      0.79      0.76      3115\n\n    accuracy                           0.76      6250\n   macro avg       0.76      0.76      0.76      6250\nweighted avg       0.76      0.76      0.76      6250\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**GaussianNB (TFIDF & BoW)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_model = Pipeline([\n    (\"preprocessor\", tfidf_prepipeline),\n    (\"svd\", TruncatedSVD(n_components=100, n_iter=7, random_state=42)),\n    (\"model\", GaussianNB())\n])\n\nnb_model2 = Pipeline([\n    (\"preprocessor\", bow_prepipeline),\n    (\"svd\", TruncatedSVD(n_components=100, n_iter=7, random_state=42)),\n    (\"model\", GaussianNB())\n])","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_model.fit(X_train, Y_train)\nY_pred = nb_model.predict(X_test)\nnbacc = accuracy_score(Y_test, Y_pred)\nnbfs = f1_score(Y_test, Y_pred,pos_label=1)\nprint(classification_report(Y_test, Y_pred))","execution_count":72,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.84      0.79      0.82      3135\n           1       0.80      0.85      0.82      3115\n\n    accuracy                           0.82      6250\n   macro avg       0.82      0.82      0.82      6250\nweighted avg       0.82      0.82      0.82      6250\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_model2.fit(X_train, Y_train)\nY_pred = nb_model2.predict(X_test)\nnbacc2 = accuracy_score(Y_test, Y_pred)\nnbfs2 = f1_score(Y_test, Y_pred,pos_label=1)\nprint(classification_report(Y_test, Y_pred))","execution_count":73,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.55      0.82      0.66      3135\n           1       0.63      0.31      0.42      3115\n\n    accuracy                           0.57      6250\n   macro avg       0.59      0.57      0.54      6250\nweighted avg       0.59      0.57      0.54      6250\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Result table**"},{"metadata":{},"cell_type":"markdown","source":"Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [['TFIDF', lracc, rfacc, nbacc], ['BoW', lracc2, rfacc2, nbacc2]]\npd.DataFrame(data, columns=[\" \", \"Logistic Regression\", \"RandomForest\", \"Naive Bayes\"])","execution_count":74,"outputs":[{"output_type":"execute_result","execution_count":74,"data":{"text/plain":"          Logistic Regression  RandomForest  Naive Bayes\n0  TFIDF              0.87024       0.81472      0.82080\n1    BoW              0.86144       0.75584      0.56752","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>Logistic Regression</th>\n      <th>RandomForest</th>\n      <th>Naive Bayes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TFIDF</td>\n      <td>0.87024</td>\n      <td>0.81472</td>\n      <td>0.82080</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BoW</td>\n      <td>0.86144</td>\n      <td>0.75584</td>\n      <td>0.56752</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"F1 score"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [['TFIDF', lrfs, rffs, nbfs], ['BoW', lrfs2, rffs2, nbfs2]]\npd.DataFrame(data, columns=[\" \", \"Logistic Regression\", \"RandomForest\", \"Naive Bayes\"])","execution_count":75,"outputs":[{"output_type":"execute_result","execution_count":75,"data":{"text/plain":"          Logistic Regression  RandomForest  Naive Bayes\n0  TFIDF             0.872062      0.818039     0.824945\n1    BoW             0.862234      0.764142     0.420330","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>Logistic Regression</th>\n      <th>RandomForest</th>\n      <th>Naive Bayes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TFIDF</td>\n      <td>0.872062</td>\n      <td>0.818039</td>\n      <td>0.824945</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BoW</td>\n      <td>0.862234</td>\n      <td>0.764142</td>\n      <td>0.420330</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Retrieve keywords for categories in reviews**"},{"metadata":{"trusted":true},"cell_type":"code","source":"aspect_terms_keywords = []\nfor x in tqdm(range(len(reviews['review']))):\n    amod = []\n    advmod = []\n    xcomp = []\n    neg = []\n    compound = []\n    if len(str(reviews['review'][x])) != 0:\n        text = str(reviews['review'][x])\n        text = BeautifulSoup(ihtml.unescape(text)).text\n        # manual list of stopwords to be removed manually\n        lines = text.replace('*','').replace('-','').replace('also ','').replace('Also ','').replace('so ','').replace('be ','').replace('are ','').replace('It ','').replace('it ','').replace('just ','').replace('get ','').replace('were ','').replace('Would ','').replace('would ','').replace('When ','').replace('when ','').replace('This ','').replace('this ','').replace('That ','').replace('that ','').replace('again ','').replace('where ','').replace('how ','').replace('has ','').replace('Here ','').replace('here ','').replace('now ','').replace('see ','').replace('why ','').split('.')              \n        for line in lines:\n            doc = nlp(line)\n            str1=''\n            str2=''\n            for token in doc:\n                if token.pos_ is 'NOUN':\n                    for j in token.lefts:\n                        if j.dep_ == 'compound':\n                            compound.append((j.text+' '+token.text,token.text))\n                        if j.dep_ is 'amod' and j.pos_ is 'ADJ':\n                            str1 = j.text+' '+token.text\n                            amod.append(j.text+' '+token.text)\n                            for k in j.lefts:\n                                if k.dep_ is 'advmod':\n                                    str2 = k.text+' '+j.text+' '+token.text\n                                    amod.append(k.text+' '+j.text+' '+token.text)\n                            match = re.search(re.escape(str1),re.escape(str2))\n                            if match is not None:\n                                amod.remove(str1)\n                if token.pos_ is 'VERB':\n                    for j in token.lefts:\n                        if j.dep_ is 'advmod' and j.pos_ is 'ADV':\n                            advmod.append(j.text+' '+token.text)\n                        if j.dep_ is 'neg' and j.pos_ is 'ADV':\n                            neg.append(j.text+' '+token.text)\n                    for j in token.rights:\n                        if j.dep_ is 'advmod'and j.pos_ is 'ADV':\n                            advmod.append(token.text+' '+j.text)\n                if token.pos_ is 'ADJ':\n                    for j,h in zip(token.rights,token.lefts):\n                        if j.dep_ is 'xcomp' and h.dep_ is not 'neg':\n                            for k in j.lefts:\n                                if k.dep_ is 'aux':\n                                    xcomp.append(token.text+' '+k.text+' '+j.text)\n                        elif j.dep_ is 'xcomp' and h.dep_ is 'neg':\n                            if k.dep_ is 'aux':\n                                    neg.append(h.text +' '+token.text+' '+k.text+' '+j.text)\n        pairs = list(set(amod+advmod+neg+xcomp))\n        for i in range(len(pairs)):\n            if len(compound)!=0:\n                for comp in compound:\n                    match = re.search(re.escape(comp[1]),re.escape(pairs[i]))\n                    if match is not None:\n                        pairs[i] = pairs[i].replace(match.group(),comp[0])        \n    aspect_terms_keywords.append(pairs)\nreviews['aspect_keywords'] = aspect_terms_keywords\nreviews.head()","execution_count":76,"outputs":[{"output_type":"stream","text":"100%|██████████| 25000/25000 [1:00:21<00:00,  6.90it/s]\n","name":"stderr"},{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"                                                  review  label         file  \\\n21492  Just plain good old stupid. <br /><br />I mean...      0   9552_1.txt   \n9488   There are too many new styles of the sitcom bu...      1   3158_8.txt   \n16933  Contrary to most other comments about \"Syriana...      0  12461_1.txt   \n12604  I'm surprised this movie is rated so highly, a...      0  12189_4.txt   \n8222   Sergeant Ryker is accused of being a traitor d...      1   8584_8.txt   \n\n                                         aspect_keywords  \n21492  [real treat, really liked, fine china, went ba...  \n9488   [n't kwhat, little bphony, stylish thriller, b...  \n16933  [great way, whole film, pure comedy, bad secti...  \n12604  [gets nowadays, rarely gets, visible flaws, to...  \n8222   [other people, once think, is all, second time...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n      <th>file</th>\n      <th>aspect_keywords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21492</th>\n      <td>Just plain good old stupid. &lt;br /&gt;&lt;br /&gt;I mean...</td>\n      <td>0</td>\n      <td>9552_1.txt</td>\n      <td>[real treat, really liked, fine china, went ba...</td>\n    </tr>\n    <tr>\n      <th>9488</th>\n      <td>There are too many new styles of the sitcom bu...</td>\n      <td>1</td>\n      <td>3158_8.txt</td>\n      <td>[n't kwhat, little bphony, stylish thriller, b...</td>\n    </tr>\n    <tr>\n      <th>16933</th>\n      <td>Contrary to most other comments about \"Syriana...</td>\n      <td>0</td>\n      <td>12461_1.txt</td>\n      <td>[great way, whole film, pure comedy, bad secti...</td>\n    </tr>\n    <tr>\n      <th>12604</th>\n      <td>I'm surprised this movie is rated so highly, a...</td>\n      <td>0</td>\n      <td>12189_4.txt</td>\n      <td>[gets nowadays, rarely gets, visible flaws, to...</td>\n    </tr>\n    <tr>\n      <th>8222</th>\n      <td>Sergeant Ryker is accused of being a traitor d...</td>\n      <td>1</td>\n      <td>8584_8.txt</td>\n      <td>[other people, once think, is all, second time...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Polarities of keywords, sentences and overall classification**"},{"metadata":{"trusted":true},"cell_type":"code","source":"analyser = SentimentIntensityAnalyzer()","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_sentiment = []\nfor i in range(len(reviews)):\n    tag_score={'pos':0,'neg':0}\n    if len(reviews['aspect_keywords'][i])!=0: \n        for phrases in reviews['aspect_keywords'][i]:\n            sent = analyser.polarity_scores(phrases)\n            tag_score['neg'] += sent['neg']\n            tag_score['pos'] += sent['pos']\n        predict_sentiment.append(max(tag_score.items(), key=operator.itemgetter(1))[0])\n    else:\n        predict_sentiment.append('neg')\nreviews['predicted_sentiment'] = predict_sentiment\nreviews.head()","execution_count":78,"outputs":[{"output_type":"execute_result","execution_count":78,"data":{"text/plain":"                                                  review  label         file  \\\n21492  Just plain good old stupid. <br /><br />I mean...      0   9552_1.txt   \n9488   There are too many new styles of the sitcom bu...      1   3158_8.txt   \n16933  Contrary to most other comments about \"Syriana...      0  12461_1.txt   \n12604  I'm surprised this movie is rated so highly, a...      0  12189_4.txt   \n8222   Sergeant Ryker is accused of being a traitor d...      1   8584_8.txt   \n\n                                         aspect_keywords predicted_sentiment  \n21492  [real treat, really liked, fine china, went ba...                 pos  \n9488   [n't kwhat, little bphony, stylish thriller, b...                 neg  \n16933  [great way, whole film, pure comedy, bad secti...                 pos  \n12604  [gets nowadays, rarely gets, visible flaws, to...                 neg  \n8222   [other people, once think, is all, second time...                 neg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n      <th>file</th>\n      <th>aspect_keywords</th>\n      <th>predicted_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21492</th>\n      <td>Just plain good old stupid. &lt;br /&gt;&lt;br /&gt;I mean...</td>\n      <td>0</td>\n      <td>9552_1.txt</td>\n      <td>[real treat, really liked, fine china, went ba...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>9488</th>\n      <td>There are too many new styles of the sitcom bu...</td>\n      <td>1</td>\n      <td>3158_8.txt</td>\n      <td>[n't kwhat, little bphony, stylish thriller, b...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>16933</th>\n      <td>Contrary to most other comments about \"Syriana...</td>\n      <td>0</td>\n      <td>12461_1.txt</td>\n      <td>[great way, whole film, pure comedy, bad secti...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>12604</th>\n      <td>I'm surprised this movie is rated so highly, a...</td>\n      <td>0</td>\n      <td>12189_4.txt</td>\n      <td>[gets nowadays, rarely gets, visible flaws, to...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>8222</th>\n      <td>Sergeant Ryker is accused of being a traitor d...</td>\n      <td>1</td>\n      <td>8584_8.txt</td>\n      <td>[other people, once think, is all, second time...</td>\n      <td>neg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = []\nfor i in reviews['label']:\n    if i == 1:\n        pos.append('pos')\n    else:\n        pos.append('neg')\nreviews['actual_sentiment'] = pos\nreviews.head()","execution_count":79,"outputs":[{"output_type":"execute_result","execution_count":79,"data":{"text/plain":"                                                  review  label         file  \\\n21492  Just plain good old stupid. <br /><br />I mean...      0   9552_1.txt   \n9488   There are too many new styles of the sitcom bu...      1   3158_8.txt   \n16933  Contrary to most other comments about \"Syriana...      0  12461_1.txt   \n12604  I'm surprised this movie is rated so highly, a...      0  12189_4.txt   \n8222   Sergeant Ryker is accused of being a traitor d...      1   8584_8.txt   \n\n                                         aspect_keywords predicted_sentiment  \\\n21492  [real treat, really liked, fine china, went ba...                 pos   \n9488   [n't kwhat, little bphony, stylish thriller, b...                 neg   \n16933  [great way, whole film, pure comedy, bad secti...                 pos   \n12604  [gets nowadays, rarely gets, visible flaws, to...                 neg   \n8222   [other people, once think, is all, second time...                 neg   \n\n      actual_sentiment  \n21492              neg  \n9488               pos  \n16933              neg  \n12604              neg  \n8222               pos  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n      <th>file</th>\n      <th>aspect_keywords</th>\n      <th>predicted_sentiment</th>\n      <th>actual_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21492</th>\n      <td>Just plain good old stupid. &lt;br /&gt;&lt;br /&gt;I mean...</td>\n      <td>0</td>\n      <td>9552_1.txt</td>\n      <td>[real treat, really liked, fine china, went ba...</td>\n      <td>pos</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>9488</th>\n      <td>There are too many new styles of the sitcom bu...</td>\n      <td>1</td>\n      <td>3158_8.txt</td>\n      <td>[n't kwhat, little bphony, stylish thriller, b...</td>\n      <td>neg</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>16933</th>\n      <td>Contrary to most other comments about \"Syriana...</td>\n      <td>0</td>\n      <td>12461_1.txt</td>\n      <td>[great way, whole film, pure comedy, bad secti...</td>\n      <td>pos</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>12604</th>\n      <td>I'm surprised this movie is rated so highly, a...</td>\n      <td>0</td>\n      <td>12189_4.txt</td>\n      <td>[gets nowadays, rarely gets, visible flaws, to...</td>\n      <td>neg</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>8222</th>\n      <td>Sergeant Ryker is accused of being a traitor d...</td>\n      <td>1</td>\n      <td>8584_8.txt</td>\n      <td>[other people, once think, is all, second time...</td>\n      <td>neg</td>\n      <td>pos</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('accuracy')\nabsaacc = accuracy_score(reviews.actual_sentiment, reviews.predicted_sentiment)\nprint(absaacc)\nprint('f1 score')\nabsafs = f1_score(reviews.actual_sentiment, reviews.predicted_sentiment,pos_label='pos')\nprint(absafs)\nprint('recall')\nabsar = recall_score(reviews.actual_sentiment, reviews.predicted_sentiment,pos_label='pos')\nprint(absar)\nprint('precision')\nabsap = precision_score(reviews.actual_sentiment, reviews.predicted_sentiment,pos_label='pos')\nprint(absap)","execution_count":80,"outputs":[{"output_type":"stream","text":"accuracy\n0.49944\nf1 score\n0.5779426644182125\nrecall\n0.68544\nprecision\n0.4995918367346939\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**Comparison**"},{"metadata":{},"cell_type":"markdown","source":"TFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [[lracc, rfacc, nbacc, absaacc]]\npd.DataFrame(data, columns=[\"Logistic Regression\", \"RandomForest\", \"Naive Bayes\", \"ABSA\"])","execution_count":81,"outputs":[{"output_type":"execute_result","execution_count":81,"data":{"text/plain":"   Logistic Regression  RandomForest  Naive Bayes     ABSA\n0              0.87024       0.81472       0.8208  0.49944","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Logistic Regression</th>\n      <th>RandomForest</th>\n      <th>Naive Bayes</th>\n      <th>ABSA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.87024</td>\n      <td>0.81472</td>\n      <td>0.8208</td>\n      <td>0.49944</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [[lrfs, rffs, nbfs, absafs]]\npd.DataFrame(data, columns=[\"Logistic Regression\", \"RandomForest\", \"Naive Bayes\", \"ABSA\"])","execution_count":82,"outputs":[{"output_type":"execute_result","execution_count":82,"data":{"text/plain":"   Logistic Regression  RandomForest  Naive Bayes      ABSA\n0             0.872062      0.818039     0.824945  0.577943","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Logistic Regression</th>\n      <th>RandomForest</th>\n      <th>Naive Bayes</th>\n      <th>ABSA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.872062</td>\n      <td>0.818039</td>\n      <td>0.824945</td>\n      <td>0.577943</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"BoW"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [[lracc2, rfacc2, nbacc2, absaacc]]\npd.DataFrame(data, columns=[\"Logistic Regression\", \"RandomForest\", \"Naive Bayes\", \"ABSA\"])","execution_count":83,"outputs":[{"output_type":"execute_result","execution_count":83,"data":{"text/plain":"   Logistic Regression  RandomForest  Naive Bayes     ABSA\n0              0.86144       0.75584      0.56752  0.49944","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Logistic Regression</th>\n      <th>RandomForest</th>\n      <th>Naive Bayes</th>\n      <th>ABSA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.86144</td>\n      <td>0.75584</td>\n      <td>0.56752</td>\n      <td>0.49944</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = [[lrfs2, rffs2, nbfs2, absafs]]\npd.DataFrame(data, columns=[\"Logistic Regression\", \"RandomForest\", \"Naive Bayes\", \"ABSA\"])","execution_count":84,"outputs":[{"output_type":"execute_result","execution_count":84,"data":{"text/plain":"   Logistic Regression  RandomForest  Naive Bayes      ABSA\n0             0.862234      0.764142      0.42033  0.577943","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Logistic Regression</th>\n      <th>RandomForest</th>\n      <th>Naive Bayes</th>\n      <th>ABSA</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.862234</td>\n      <td>0.764142</td>\n      <td>0.42033</td>\n      <td>0.577943</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}